{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Общее-впечатление\" data-toc-modified-id=\"Общее-впечатление-0.1\"><font color=\"orange\">Общее впечатление</font></a></span></li><li><span><a href=\"#Общее-впечатление-(ревью-2)\" data-toc-modified-id=\"Общее-впечатление-(ревью-2)-0.2\"><font color=\"orange\">Общее впечатление (ревью 2)</font></a></span></li></ul></li><li><span><a href=\"#Шаг-1.-Подготовка-и-загрузка-данных.\" data-toc-modified-id=\"Шаг-1.-Подготовка-и-загрузка-данных.-1\">Шаг 1. Подготовка и загрузка данных.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Шаг-1.1.-Загрузка-и-обзор-данных.\" data-toc-modified-id=\"Шаг-1.1.-Загрузка-и-обзор-данных.-1.1\">Шаг 1.1. Загрузка и обзор данных.</a></span></li><li><span><a href=\"#Шаг-1.2.-Очистка-текста-и-лемматизация.\" data-toc-modified-id=\"Шаг-1.2.-Очистка-текста-и-лемматизация.-1.2\">Шаг 1.2. Очистка текста и лемматизация.</a></span></li></ul></li><li><span><a href=\"#Шаг-2.-Обучение-и-тестирование-моделей.\" data-toc-modified-id=\"Шаг-2.-Обучение-и-тестирование-моделей.-2\">Шаг 2. Обучение и тестирование моделей.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Шаг-2.1.-Подготовка-данных-для-обучения-и-тестирования-моделей.\" data-toc-modified-id=\"Шаг-2.1.-Подготовка-данных-для-обучения-и-тестирования-моделей.-2.1\">Шаг 2.1. Подготовка данных для обучения и тестирования моделей.</a></span></li><li><span><a href=\"#Шаг-2.2.-Обучение-и-тестирование-моделей.\" data-toc-modified-id=\"Шаг-2.2.-Обучение-и-тестирование-моделей.-2.2\">Шаг 2.2. Обучение и тестирование моделей.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Обучение-и-Тестирование-моделей-на-несбалансированной-обучающей-выборке.\" data-toc-modified-id=\"Обучение-и-Тестирование-моделей-на-несбалансированной-обучающей-выборке.-2.2.1\">Обучение и Тестирование моделей на несбалансированной обучающей выборке.</a></span></li><li><span><a href=\"#Обучение-и-Тестирование-моделей-на-сбалансированной-обучающей-выборке.\" data-toc-modified-id=\"Обучение-и-Тестирование-моделей-на-сбалансированной-обучающей-выборке.-2.2.2\">Обучение и Тестирование моделей на сбалансированной обучающей выборке.</a></span></li><li><span><a href=\"#Проверка-моделей-на-тестовой-выборке.\" data-toc-modified-id=\"Проверка-моделей-на-тестовой-выборке.-2.2.3\">Проверка моделей на тестовой выборке.</a></span></li></ul></li></ul></li><li><span><a href=\"#Шаг-3.-Итоги-исследования.\" data-toc-modified-id=\"Шаг-3.-Итоги-исследования.-3\">Шаг 3. Итоги исследования.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Шаг-3.1.-Описание-хода-исследования.\" data-toc-modified-id=\"Шаг-3.1.-Описание-хода-исследования.-3.1\">Шаг 3.1. Описание хода исследования.</a></span></li><li><span><a href=\"#Шаг-3.2.-Общий-вывод.\" data-toc-modified-id=\"Шаг-3.2.-Общий-вывод.-3.2\">Шаг 3.2. Общий вывод.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Необходимо обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Нужно построиить модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузка и подготовка данных.\n",
    "2. Обучение разных моделей. \n",
    "3. Выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но можно попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1. Подготовка и загрузка данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1.1. Загрузка и обзор данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт библиотеки для предупреждений\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# импорт основных библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# импорт библиотек с моделями\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# импорт pipline, GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "# импорт метрик качества\n",
    "from sklearn.metrics import f1_score, make_scorer, classification_report\n",
    "# импорт инструментов для обработки выборок\n",
    "from sklearn.utils import shuffle\n",
    "from random import sample\n",
    "from scipy import sparse\n",
    "# импорт инструментов для обработки текста\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "# импорт библиотеки замера времени\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\regki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\regki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\regki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\regki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# импорт и загрузка инструментов для работы с текстом\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# установка ширины вывода столбцов\n",
    "pd.set_option('max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when your view completely contradicts the coverage in reliable sources, why should anyone care what you f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is a horrible thing you put on my talk page.  128.61.19.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for prostitution ring.  - Crunch Captain.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put on the speedy to have the first version deleted now that I look at it.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                         text  \\\n",
       "0       Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I ...   \n",
       "1                                            D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2       Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits in...   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of...   \n",
       "4                                                                                         You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "...                                                                                                                                                       ...   \n",
       "159566  \":::::And for the second time of asking, when your view completely contradicts the coverage in reliable sources, why should anyone care what you f...   \n",
       "159567                                                  You should be ashamed of yourself \\n\\nThat is a horrible thing you put on my talk page.  128.61.19.93   \n",
       "159568                                                                    Spitzer \\n\\nUmm, theres no actual article for prostitution ring.  - Crunch Captain.   \n",
       "159569                                   And it looks like it was actually you who put on the speedy to have the first version deleted now that I look at it.   \n",
       "159570  \"\\nAnd ... I really don't think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" g...   \n",
       "\n",
       "        toxic  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "159566      0  \n",
       "159567      0  \n",
       "159568      0  \n",
       "159569      0  \n",
       "159570      0  \n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# чтение файла с датасетом, испульзуя конструкцию try-except и вывод таблицы на экран\n",
    "try:\n",
    "    df = pd.read_csv('/Users/regki/Downloads/toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# обзор данных\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- В данных 159571 строка без пропусков.\n",
    "- Два столбца: один содержит целевой признак в целочисленном формате, другой включает данные в строковом формате.\n",
    "- Посмотрим данные на дисбалланс целевого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля значений класса \"0\" в целевом признаке: 89.83%\n",
      "Доля значений класса \"1\" в целевом признаке: 10.17%\n"
     ]
    }
   ],
   "source": [
    "# вывод на экран доли каждого из классов целевого признака\n",
    "for i in range(2):\n",
    "    print('Доля значений класса \"{}\" в целевом признаке: {:.2%}'.format(i, len(df[df['toxic'] == i])/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целевом признаке наблюдается сильный дисбалланс. Попробуем в дальнейшем как вариант обучить модели на сбалансированной тренировочной выборке. Будем уменьшать выборку, так как увеличение приведет к дублированию одних и тех же текстовых данных, что может привести к переобучению моделей.\n",
    "\n",
    "Создадим функцию, которая уменьшает число доминирующего класса, она принимает признаки и целевой признак и аргумент \"fraction\". Функция возвращает случаеные элементы в таком количестве, чтобы их доля от исходных данных была равна \"fraction\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание функции для \"даунсэмплинга\"\n",
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    features_downsampled = pd.concat(\n",
    "    [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat(\n",
    "    [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "    features_downsampled, target_downsampled, random_state=12345)\n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1.2. Очистка текста и лемматизация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая принимает аргументом текст, а возвращает очищенный текст от символов и знаков препинания и стоп слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть некоторые слова, которые можно добавить в список \"стоп-слов\". Создадим функцию для добавления новых стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание функции для обновления списка \"стоп-слов\"\n",
    "def add_stopword(list_stop_word):\n",
    "    for i in list_stop_word:\n",
    "        stopwords.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stop_words = ['i\\'m', 'he\\'s', 'that\\'s', 'this\\'s', 'be\\'ll', 'can\\'t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stopword(list_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание функции для очистки текста\n",
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', \" \", text)\n",
    "    re_text = \" \".join([word for word in text.split() if word not in stopwords])\n",
    "    return re_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits made username hardcore metallica fan reverted vandalisms closure gas voted new york dolls fac please remove template talk page s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww matches background colour seemingly stuck thanks talk january utc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>second time asking view completely contradicts coverage reliable sources anyone care feel even give consistent argument opening supposed mention s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>spitzer umm theres actual article prostitution ring crunch captain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>looks like actually put speedy first version deleted look</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>really think understand came idea bad right away kind community goes bad ideas go away instead helping rewrite</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                         text  \\\n",
       "0       explanation edits made username hardcore metallica fan reverted vandalisms closure gas voted new york dolls fac please remove template talk page s...   \n",
       "1                                                                                       aww matches background colour seemingly stuck thanks talk january utc   \n",
       "2               hey man really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info   \n",
       "3       make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie d...   \n",
       "4                                                                                                                               sir hero chance remember page   \n",
       "...                                                                                                                                                       ...   \n",
       "159566  second time asking view completely contradicts coverage reliable sources anyone care feel even give consistent argument opening supposed mention s...   \n",
       "159567                                                                                                                   ashamed horrible thing put talk page   \n",
       "159568                                                                                     spitzer umm theres actual article prostitution ring crunch captain   \n",
       "159569                                                                                              looks like actually put speedy first version deleted look   \n",
       "159570                                         really think understand came idea bad right away kind community goes bad ideas go away instead helping rewrite   \n",
       "\n",
       "        toxic  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "159566      0  \n",
       "159567      0  \n",
       "159568      0  \n",
       "159569      0  \n",
       "159570      0  \n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# применение функции к столбцу с текстом\n",
    "df['text'] = df['text'].apply(clear_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим две функции. Первая расставляет тэги у различных частей речи. Вторая, принимает аргументом текст, внутри, используя лемматизатор и функцию-тегов лемматизирует текст, возвращая исходную форму частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание функции для pos-тегов\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.VERB\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция лемматизации\n",
    "def lemm_txt(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            lemmatized_sentence.append(wnl.lemmatize(word, tag))\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание \"корпуса с текстами\" \n",
    "corpus = df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['explanation edits made username hardcore metallica fan reverted vandalisms closure gas voted new york dolls fac please remove template talk page since retired',\n",
       "       'aww matches background colour seemingly stuck thanks talk january utc',\n",
       "       'hey man really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info',\n",
       "       ...,\n",
       "       'spitzer umm theres actual article prostitution ring crunch captain',\n",
       "       'looks like actually put speedy first version deleted look',\n",
       "       'really think understand came idea bad right away kind community goes bad ideas go away instead helping rewrite'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вывод на экран корпуса\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation edits make username hardcore metallica fan revert vandalisms closure gas vote new york doll fac please remove template talk page since retire',\n",
       " 'aww match background colour seemingly stick thanks talk january utc',\n",
       " 'hey man really try edit war guy constantly remove relevant information talk edits instead talk page seem care format actual info',\n",
       " 'make real suggestion improvement wonder section statistic later subsection type accident think reference may need tidy exact format ie date format etc later one else first preference format style reference want please let know appear backlog article review guess may delay reviewer turn list relevant form eg wikipedia good article nomination transport',\n",
       " 'sir hero chance remember page',\n",
       " 'congratulation well use tool well talk',\n",
       " 'cocksucker piss around work',\n",
       " 'vandalism matt shirvington article revert please ban',\n",
       " 'sorry word nonsense offensive anyway intend write anything article wow would jump vandalism merely request encyclopedic one use school reference selective breeding page almost stub point animal breed short messy article give info must someone around expertise eugenics',\n",
       " 'alignment subject contrary dulithgow']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# применение функции лемматизации к корпусу\n",
    "corpus = [lemm_txt(corpus[i]) for i in range(len(corpus))]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка типа данных корпуса\n",
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвертация данных корпуса в формат массива\n",
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка типа данных корпуса\n",
    "type(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2. Обучение и тестирование моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2.1. Подготовка данных для обучения и тестирования моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запись переменных с признаками и целевым признаком\n",
    "X = corpus\n",
    "y = df['toxic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# деление даннх на обучающую и валидационную + тестовую выборки\n",
    "X_tr, X_v_t, y_train, y_v_t = train_test_split(X, y, test_size=0.4, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# деление данных на валидационную и тестовые выборки\n",
    "X_val, X_tst, y_valid, y_test = train_test_split(X_v_t, y_v_t, test_size=0.5, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для проверки размеров выборок\n",
    "def sample_shape(train, train_dn, valid, test):\n",
    "    print(f'Размер тренировочной выборки:{train.shape}\\n'\n",
    "          f'Размер тренировочной выборки:{train_dn.shape}\\n'\n",
    "          f'Размер валидационной выборки:{valid.shape}\\n'\n",
    "          f'Размер тестовой выборки:{test.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим сбалансированную выборку с помощью функции \"даунсэмплинга\". Прежде тренировочную выборку переведем в формат \"Series\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# перевод тренировочных данных в формат \"Series\"\n",
    "X_tr = pd.Series(X_tr)\n",
    "y_train = pd.Series(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               happy new year eurocargt class plainlinks eurocargt prosperous productive enjoyable new year thanks contribution wikipedia\n",
       "1    speedy deletion mahur malie masahet tag place mahur malie masahet request speedily delete wikipedia do section criterion speedy deletion short art...\n",
       "2                                                                                                                  barnstar real life barnstar let us star\n",
       "3                                                                                                                           article self appoint caretaker\n",
       "4    review add paste http theearlyregistration com album review carrie lowell sufjan stevens http www thefourohfive com review article sufjan stevens ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вывод на экран верхней части тренировочного \"Series\"\n",
    "X_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применение \"даунсэмплинга\" к тренировочным данным\n",
    "X_tr_dn, y_train_dn = downsample(X_tr, y_train, len(y_train[y_train == 1])/len(y_train[y_train == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# перевод тренировочных данных в формат массива\n",
    "X_tr = np.array(X_tr)\n",
    "y_train = np.array(y_train)\n",
    "X_tr_dn = np.array(X_tr_dn)\n",
    "y_train_dn = np.array(y_train_dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочной выборки:(95742,)\n",
      "Размер тренировочной выборки:(19506,)\n",
      "Размер валидационной выборки:(31914,)\n",
      "Размер тестовой выборки:(31915,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# вывод на экран размера выборок\n",
    "sample_shape(X_tr, X_tr_dn, X_val, X_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем TF-IDF векторизацию текстовых признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF векторизация признаков\n",
    "tf_idf = TfidfVectorizer(dtype=np.float32, stop_words=stopwords)\n",
    "X_train = tf_idf.fit_transform(X_tr)\n",
    "X_train_dn = tf_idf.transform(X_tr_dn)\n",
    "X_valid = tf_idf.transform(X_val)\n",
    "X_test = tf_idf.transform(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочной выборки:(95742, 113883)\n",
      "Размер тренировочной выборки:(19506, 113883)\n",
      "Размер валидационной выборки:(31914, 113883)\n",
      "Размер тестовой выборки:(31915, 113883)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# вывод на экран размера векторизированных выборок\n",
    "sample_shape(X_train, X_train_dn, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2.2. Обучение и тестирование моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения поставленной задачи выберем 4 модели МО:\n",
    "- Dummy-модель \"DummyClassifier\".\n",
    "- Линейная модель \"LogisticRegression\".\n",
    "- Модель опорных векторов \"SupportVectorMachine\".\n",
    "- Модель градиентного бустинга \"LGBMClassifier\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание \"pipeline\" для пяти моделей МО\n",
    "pipe_dumc = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', DummyClassifier())])\n",
    "\n",
    "pipe_lr = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LogisticRegression(random_state=12345))])\n",
    "                     \n",
    "pipe_svc = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf',  SVC(random_state=12345))])                     \n",
    "\n",
    "pipe_lgbm = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', LGBMClassifier(random_state=12345))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание списков-словарей с параметрами моделей МО\n",
    "\n",
    "param_dumc = {'clf__strategy': ['most_frequent', 'prior', 'stratified', 'uniform', 'constant']}\n",
    "\n",
    "param_lr = { 'clf__penalty' : ['l1', 'l2'],\n",
    "             'clf__solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "param_svc = {'clf__max_iter' : [50, 100],\n",
    "             'clf__gamma': ['scale', 'auto'],\n",
    "             'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "param_lgbm = {'clf__max_depth': [12, 25],\n",
    "             'clf__learning_rate' : [0.2, 0.3],\n",
    "             'clf__n_estimators' : [200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание переменных с записью в них GridSearchCV (далее GSCV) для поиска оптимальных параметров по каждой модели МО\n",
    "\n",
    "\n",
    "DUMC = GridSearchCV(estimator=pipe_dumc,\n",
    "            param_grid=param_dumc,\n",
    "            scoring=make_scorer(f1_score, average='macro'),\n",
    "            refit='f1_score',\n",
    "            n_jobs=-1,       \n",
    "            cv=2) \n",
    "\n",
    "LR = GridSearchCV(estimator=pipe_lr,\n",
    "            param_grid=param_lr,\n",
    "            scoring=make_scorer(f1_score, average='macro'),\n",
    "            refit='f1_score',\n",
    "            n_jobs=-1,\n",
    "            cv=2) \n",
    "\n",
    "SVC = GridSearchCV(estimator=pipe_svc,\n",
    "            param_grid=param_svc,      \n",
    "            scoring=make_scorer(f1_score, average='macro'),\n",
    "            refit='f1_score',\n",
    "            n_jobs=-1,\n",
    "            cv=2)\n",
    "\n",
    "LGBM = GridSearchCV(estimator=pipe_lgbm,\n",
    "            param_grid=param_lgbm,\n",
    "            scoring=make_scorer(f1_score, average='macro'),\n",
    "            refit='f1_score',\n",
    "            n_jobs=-1,\n",
    "            cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание списка с GridSearchCV(GSCV) по каждой модели МО\n",
    "grids = [DUMC, LR,  SVC, LGBM]\n",
    "\n",
    "# создание словаря с наименованием каждой модели МО\n",
    "grid_dict = {0: 'DummyClassifier',\n",
    "             1: 'LogisticRegression', \n",
    "             2: 'SupportVectorMachine',\n",
    "             3: 'LGBMClassifier'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию:\n",
    "- Принимает аргументами: тренировочную и тестовую выборки(отдельно с признаками и целевым признаком). А также именованные аргументы: с нулевым значением F1-score(с которым в дальнейшем в цикле сравнивается значение F1 по предсказанию модели), нулевое значение лучшей модели и ее индекса, наименование выборки на которой производится предсказание(валидационной или тестовой). \n",
    "- Внутри функции:\n",
    "  - Выводятся на экран параметры модели, при которых достигается лучшее значение F1-score.\n",
    "  - Вычисляется время работы по обучению и предсказанию модели.\n",
    "  - Далее параметры модели, время работы и лучшее значение F1-score добавляются в словарь.\n",
    "  - Вывод на экран модели с лучшим значением F1-score.\n",
    "- Функция возвращает словарь с параметрами, временем обучения и предсказания, а также F1-score для всех моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание функции для выбора модели с лучшим RMSE\n",
    "def choose_model(list_grids, feature_tr, target_tr, feature_tst, target_tst, best_score, best_reg, best_gs, name_sample):\n",
    "    best_model_params = {'Модель' : [],\n",
    "                        'Лучшие параметры модели' : [],\n",
    "                        'Лучшее значение F1-score модели' : [],\n",
    "                        'Время обучения модели' : [],\n",
    "                        'Время предсказания модели' : []}\n",
    "    for idx, gs in enumerate(list_grids):\n",
    "        print('Модель:{}'.format(grid_dict[idx]))\n",
    "        print()\n",
    "        t0 = time()\n",
    "        gs.fit(feature_tr, target_tr)\n",
    "        time_train = time()-t0\n",
    "        print('Лучшие параметры модели:', gs.best_params_)\n",
    "        print()\n",
    "        t1 = time()\n",
    "        predictions = gs.predict(feature_tst)\n",
    "        time_predict = time() - t1\n",
    "        F1 = f1_score(target_tst, predictions)\n",
    "        print('F1_score для модели {} на {} выборке: {:.2f}'.format(grid_dict[idx], name_sample, F1))\n",
    "        print()\n",
    "        if F1 > best_score:\n",
    "            best_score = F1\n",
    "            best_gs = gs\n",
    "            best_reg = idx\n",
    "        time_iter = time()-t0\n",
    "        best_model_params['Модель'].append(grid_dict[idx])\n",
    "        best_model_params['Лучшие параметры модели'].append(gs.best_params_)\n",
    "        best_model_params['Лучшее значение F1-score модели'].append(F1)\n",
    "        best_model_params['Время обучения модели'].append(time_train)\n",
    "        best_model_params['Время предсказания модели'].append(time_predict)\n",
    "        print('Время обучения модели {}: {:.2f}'.format(grid_dict[idx], time_train))\n",
    "        print()\n",
    "        print('Время предсказания модели {}: {:.2f}'.format(grid_dict[idx], time_predict))\n",
    "        print('\\n\\n')\n",
    "    print('\\n')\n",
    "    print('Лучшая модель на {}: {}'.format(name_sample, grid_dict[best_reg]))\n",
    "    print('Лучший показатель F1 на {} выборке: {:.2f}'.format(name_sample, best_score))    \n",
    "    return best_model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение и Тестирование моделей на несбалансированной обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель:DummyClassifier\n",
      "\n",
      "Лучшие параметры модели: {'clf__strategy': 'stratified'}\n",
      "\n",
      "F1_score для модели DummyClassifier на валидационной выборке: 0.10\n",
      "\n",
      "Время обучения модели DummyClassifier: 12.42\n",
      "\n",
      "Время предсказания модели DummyClassifier: 1.20\n",
      "\n",
      "\n",
      "\n",
      "Модель:LogisticRegression\n",
      "\n",
      "Лучшие параметры модели: {'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "\n",
      "F1_score для модели LogisticRegression на валидационной выборке: 0.78\n",
      "\n",
      "Время обучения модели LogisticRegression: 27.37\n",
      "\n",
      "Время предсказания модели LogisticRegression: 0.95\n",
      "\n",
      "\n",
      "\n",
      "Модель:SupportVectorMachine\n",
      "\n",
      "Лучшие параметры модели: {'clf__gamma': 'scale', 'clf__kernel': 'sigmoid', 'clf__max_iter': 100}\n",
      "\n",
      "F1_score для модели SupportVectorMachine на валидационной выборке: 0.41\n",
      "\n",
      "Время обучения модели SupportVectorMachine: 50.18\n",
      "\n",
      "Время предсказания модели SupportVectorMachine: 1.53\n",
      "\n",
      "\n",
      "\n",
      "Модель:LGBMClassifier\n",
      "\n",
      "Лучшие параметры модели: {'clf__learning_rate': 0.2, 'clf__max_depth': 25, 'clf__n_estimators': 200}\n",
      "\n",
      "F1_score для модели LGBMClassifier на валидационной выборке: 0.78\n",
      "\n",
      "Время обучения модели LGBMClassifier: 103.16\n",
      "\n",
      "Время предсказания модели LGBMClassifier: 1.71\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Лучшая модель на валидационной: LGBMClassifier\n",
      "Лучший показатель F1 на валидационной выборке: 0.78\n"
     ]
    }
   ],
   "source": [
    "# вызов функции на несбалансированной обучающей выборке\n",
    "dict_params_valid = choose_model(grids, X_tr, y_train, X_val, y_valid, best_score=0, best_reg=0,\n",
    "                                 best_gs='', name_sample='валидационной')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Лучшие параметры модели</th>\n",
       "      <th>Лучшее значение F1-score модели</th>\n",
       "      <th>Время обучения модели</th>\n",
       "      <th>Время предсказания модели</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>{'clf__strategy': 'stratified'}</td>\n",
       "      <td>0.099463</td>\n",
       "      <td>12.421422</td>\n",
       "      <td>1.195822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'clf__penalty': 'l1', 'clf__solver': 'liblinear'}</td>\n",
       "      <td>0.775892</td>\n",
       "      <td>27.365571</td>\n",
       "      <td>0.949582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SupportVectorMachine</td>\n",
       "      <td>{'clf__gamma': 'scale', 'clf__kernel': 'sigmoid', 'clf__max_iter': 100}</td>\n",
       "      <td>0.408441</td>\n",
       "      <td>50.183374</td>\n",
       "      <td>1.530915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'clf__learning_rate': 0.2, 'clf__max_depth': 25, 'clf__n_estimators': 200}</td>\n",
       "      <td>0.777702</td>\n",
       "      <td>103.155634</td>\n",
       "      <td>1.713048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Модель  \\\n",
       "0       DummyClassifier   \n",
       "1    LogisticRegression   \n",
       "2  SupportVectorMachine   \n",
       "3        LGBMClassifier   \n",
       "\n",
       "                                                       Лучшие параметры модели  \\\n",
       "0                                              {'clf__strategy': 'stratified'}   \n",
       "1                           {'clf__penalty': 'l1', 'clf__solver': 'liblinear'}   \n",
       "2      {'clf__gamma': 'scale', 'clf__kernel': 'sigmoid', 'clf__max_iter': 100}   \n",
       "3  {'clf__learning_rate': 0.2, 'clf__max_depth': 25, 'clf__n_estimators': 200}   \n",
       "\n",
       "   Лучшее значение F1-score модели  Время обучения модели  \\\n",
       "0                         0.099463              12.421422   \n",
       "1                         0.775892              27.365571   \n",
       "2                         0.408441              50.183374   \n",
       "3                         0.777702             103.155634   \n",
       "\n",
       "   Время предсказания модели  \n",
       "0                   1.195822  \n",
       "1                   0.949582  \n",
       "2                   1.530915  \n",
       "3                   1.713048  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание датафрейма из словаря с параметрами и результатами работы моделей на несбалансированной обучающей выборке\n",
    "pd.set_option('max_colwidth', 150)\n",
    "df_valid_param = pd.DataFrame(data=dict_params_valid)\n",
    "df_valid_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение и Тестирование моделей на сбалансированной обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель:DummyClassifier\n",
      "\n",
      "Лучшие параметры модели: {'clf__strategy': 'uniform'}\n",
      "\n",
      "F1_score для модели DummyClassifier на валидационной выборке: 0.17\n",
      "\n",
      "Время обучения модели DummyClassifier: 2.50\n",
      "\n",
      "Время предсказания модели DummyClassifier: 0.95\n",
      "\n",
      "\n",
      "\n",
      "Модель:LogisticRegression\n",
      "\n",
      "Лучшие параметры модели: {'clf__penalty': 'l2', 'clf__solver': 'saga'}\n",
      "\n",
      "F1_score для модели LogisticRegression на валидационной выборке: 0.67\n",
      "\n",
      "Время обучения модели LogisticRegression: 4.88\n",
      "\n",
      "Время предсказания модели LogisticRegression: 7.74\n",
      "\n",
      "\n",
      "\n",
      "Модель:SupportVectorMachine\n",
      "\n",
      "Лучшие параметры модели: {'clf__gamma': 'scale', 'clf__kernel': 'rbf', 'clf__max_iter': 100}\n",
      "\n",
      "F1_score для модели SupportVectorMachine на валидационной выборке: 0.36\n",
      "\n",
      "Время обучения модели SupportVectorMachine: 9.34\n",
      "\n",
      "Время предсказания модели SupportVectorMachine: 8.39\n",
      "\n",
      "\n",
      "\n",
      "Модель:LGBMClassifier\n",
      "\n",
      "Лучшие параметры модели: {'clf__learning_rate': 0.2, 'clf__max_depth': 12, 'clf__n_estimators': 200}\n",
      "\n",
      "F1_score для модели LGBMClassifier на валидационной выборке: 0.70\n",
      "\n",
      "Время обучения модели LGBMClassifier: 18.44\n",
      "\n",
      "Время предсказания модели LGBMClassifier: 1.08\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Лучшая модель на валидационной: LGBMClassifier\n",
      "Лучший показатель F1 на валидационной выборке: 0.70\n"
     ]
    }
   ],
   "source": [
    "# вызов функции на сбалансированной обучающей выборке\n",
    "dict_params_valid_dn = choose_model(grids, X_tr_dn, y_train_dn, X_val, y_valid, best_score=0, best_reg=0,\n",
    "                                 best_gs='', name_sample='валидационной')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Лучшие параметры модели</th>\n",
       "      <th>Лучшее значение F1-score модели</th>\n",
       "      <th>Время обучения модели</th>\n",
       "      <th>Время предсказания модели</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>{'clf__strategy': 'uniform'}</td>\n",
       "      <td>0.173909</td>\n",
       "      <td>2.498820</td>\n",
       "      <td>0.950598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'clf__penalty': 'l2', 'clf__solver': 'saga'}</td>\n",
       "      <td>0.670971</td>\n",
       "      <td>4.884389</td>\n",
       "      <td>7.740827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SupportVectorMachine</td>\n",
       "      <td>{'clf__gamma': 'scale', 'clf__kernel': 'rbf', 'clf__max_iter': 100}</td>\n",
       "      <td>0.360324</td>\n",
       "      <td>9.336416</td>\n",
       "      <td>8.393831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'clf__learning_rate': 0.2, 'clf__max_depth': 12, 'clf__n_estimators': 200}</td>\n",
       "      <td>0.703505</td>\n",
       "      <td>18.444561</td>\n",
       "      <td>1.079504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Модель  \\\n",
       "0       DummyClassifier   \n",
       "1    LogisticRegression   \n",
       "2  SupportVectorMachine   \n",
       "3        LGBMClassifier   \n",
       "\n",
       "                                                       Лучшие параметры модели  \\\n",
       "0                                                 {'clf__strategy': 'uniform'}   \n",
       "1                                {'clf__penalty': 'l2', 'clf__solver': 'saga'}   \n",
       "2          {'clf__gamma': 'scale', 'clf__kernel': 'rbf', 'clf__max_iter': 100}   \n",
       "3  {'clf__learning_rate': 0.2, 'clf__max_depth': 12, 'clf__n_estimators': 200}   \n",
       "\n",
       "   Лучшее значение F1-score модели  Время обучения модели  \\\n",
       "0                         0.173909               2.498820   \n",
       "1                         0.670971               4.884389   \n",
       "2                         0.360324               9.336416   \n",
       "3                         0.703505              18.444561   \n",
       "\n",
       "   Время предсказания модели  \n",
       "0                   0.950598  \n",
       "1                   7.740827  \n",
       "2                   8.393831  \n",
       "3                   1.079504  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание датафрейма из словаря с параметрами и результатами работы моделей на сбалансированной обучающей выборке\n",
    "df_valid_param_dn = pd.DataFrame(data=dict_params_valid_dn)\n",
    "df_valid_param_dn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- На сбалансированной выборке результы предсказания ведущих моделей оказались даже примерно на 10 % хуже, чем на несбалансированной.\n",
    "- Все модели показали существенно лучший результат, чем Dummy-модель.\n",
    "- Лучшие показатели F1-score достигли сразу две модели: \"LogisticRegression\" и \"LGBMClassifier\". \n",
    "- Но модель \"LogisticRegression\" обучается существенно быстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверка моделей на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель:DummyClassifier\n",
      "\n",
      "Лучшие параметры модели: {'clf__strategy': 'stratified'}\n",
      "\n",
      "F1_score для модели DummyClassifier на тестовой выборке: 0.10\n",
      "\n",
      "Время обучения модели DummyClassifier: 12.63\n",
      "\n",
      "Время предсказания модели DummyClassifier: 1.12\n",
      "\n",
      "\n",
      "\n",
      "Модель:LogisticRegression\n",
      "\n",
      "Лучшие параметры модели: {'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "\n",
      "F1_score для модели LogisticRegression на тестовой выборке: 0.77\n",
      "\n",
      "Время обучения модели LogisticRegression: 26.00\n",
      "\n",
      "Время предсказания модели LogisticRegression: 0.98\n",
      "\n",
      "\n",
      "\n",
      "Модель:SupportVectorMachine\n",
      "\n",
      "Лучшие параметры модели: {'clf__gamma': 'scale', 'clf__kernel': 'sigmoid', 'clf__max_iter': 100}\n",
      "\n",
      "F1_score для модели SupportVectorMachine на тестовой выборке: 0.41\n",
      "\n",
      "Время обучения модели SupportVectorMachine: 51.17\n",
      "\n",
      "Время предсказания модели SupportVectorMachine: 1.50\n",
      "\n",
      "\n",
      "\n",
      "Модель:LGBMClassifier\n",
      "\n",
      "Лучшие параметры модели: {'clf__learning_rate': 0.2, 'clf__max_depth': 25, 'clf__n_estimators': 200}\n",
      "\n",
      "F1_score для модели LGBMClassifier на тестовой выборке: 0.77\n",
      "\n",
      "Время обучения модели LGBMClassifier: 99.37\n",
      "\n",
      "Время предсказания модели LGBMClassifier: 1.66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Лучшая модель на тестовой: LGBMClassifier\n",
      "Лучший показатель F1 на тестовой выборке: 0.77\n"
     ]
    }
   ],
   "source": [
    "# создание датафрейма из словаря с параметрами и результатами работы моделей на тестовой выборке\n",
    "dict_params_test = choose_model(grids, X_tr, y_train, X_tst, y_test, best_score=0, best_reg=0,\n",
    "                                 best_gs='', name_sample='тестовой')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Лучшие параметры модели</th>\n",
       "      <th>Лучшее значение F1-score модели</th>\n",
       "      <th>Время обучения модели</th>\n",
       "      <th>Время предсказания модели</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>{'clf__strategy': 'stratified'}</td>\n",
       "      <td>0.102181</td>\n",
       "      <td>12.629856</td>\n",
       "      <td>1.118376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'clf__penalty': 'l1', 'clf__solver': 'liblinear'}</td>\n",
       "      <td>0.769258</td>\n",
       "      <td>26.001942</td>\n",
       "      <td>0.975572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SupportVectorMachine</td>\n",
       "      <td>{'clf__gamma': 'scale', 'clf__kernel': 'sigmoid', 'clf__max_iter': 100}</td>\n",
       "      <td>0.405564</td>\n",
       "      <td>51.173066</td>\n",
       "      <td>1.500699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'clf__learning_rate': 0.2, 'clf__max_depth': 25, 'clf__n_estimators': 200}</td>\n",
       "      <td>0.771667</td>\n",
       "      <td>99.368074</td>\n",
       "      <td>1.655230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Модель  \\\n",
       "0       DummyClassifier   \n",
       "1    LogisticRegression   \n",
       "2  SupportVectorMachine   \n",
       "3        LGBMClassifier   \n",
       "\n",
       "                                                       Лучшие параметры модели  \\\n",
       "0                                              {'clf__strategy': 'stratified'}   \n",
       "1                           {'clf__penalty': 'l1', 'clf__solver': 'liblinear'}   \n",
       "2      {'clf__gamma': 'scale', 'clf__kernel': 'sigmoid', 'clf__max_iter': 100}   \n",
       "3  {'clf__learning_rate': 0.2, 'clf__max_depth': 25, 'clf__n_estimators': 200}   \n",
       "\n",
       "   Лучшее значение F1-score модели  Время обучения модели  \\\n",
       "0                         0.102181              12.629856   \n",
       "1                         0.769258              26.001942   \n",
       "2                         0.405564              51.173066   \n",
       "3                         0.771667              99.368074   \n",
       "\n",
       "   Время предсказания модели  \n",
       "0                   1.118376  \n",
       "1                   0.975572  \n",
       "2                   1.500699  \n",
       "3                   1.655230  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание датафрейма из словаря с параметрами и результатами работы моделей на тестовой выборке\n",
    "df_test_param = pd.DataFrame(data=dict_params_test)\n",
    "df_test_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- В общем результаты тестовой выборки повторяют проверку моделей на валидационной выборке.\n",
    "- Лучший F1-score достигнут моделью градиентного бустинга \"LGBMClassifier\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3. Итоги исследования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3.1. Описание хода исследования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для реализации поставленной задачи мы:\n",
    "   - Изучили датасет, \n",
    "   - Очистили текстовые признаки от лишних символов и слов из \"стоп-списка\".\n",
    "   - Лемматизировали текстовые данные.\n",
    "   - Разделили общие признаки на обучающую, валидационную и тестовую выборки.\n",
    "   - Провели процедуру\"даунсемплинга\" для балансировки выборки по целевому признаку..\n",
    "   - Подготовили модели к обучению и проверке.\n",
    "   - Обучили модели и проверили их на валидационной и тестовой выборках.\n",
    " \n",
    " По итогам исследования делаем общий вывод:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3.2. Общий вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Лучше подготовить модели для проверки твитов на токсичность помогла несбалансировнанная тренировочная выборка.\n",
    "- Самой быстрой моделью в обучении и предсказании и по совместительству второй результат F1-score на тестовой выборке показала линейная модель \"LogisticRegression\".\n",
    "- Лучший результат F1-score на тестовой выборке достигнут моделью градиентного бустинга \"LGBMClassifier\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
